{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Machine Learning Library ü§ñ\n",
    "Very briefly, Machine Learning algorithm can be divided into two main categories; the classical one (such as linear regression, SVM, etc) and deep learning (involves neural network)\n",
    "We will be looking at each library (or maybe some of it) during a later stage of our tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Scikit-Learn\n",
    "\n",
    "Scikit-learn is a library in Python that provides many unsupervised and supervised learning algorithms. It‚Äôs built upon some of the technology you might already be familiar with, like NumPy, pandas, and Matplotlib!\n",
    "\n",
    "- There is a vast collection of ML algorithms that are all included (It‚Äôs like a repo, but a repo of ML algorithm)\n",
    "- An amazing way to get a handle of what different types of models do, as well as giving some intuition about some algorithm perform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1.1 Example\n",
    "\n",
    "For our first ML example that we will be doing, we are going to look at PENGUIN üêß. We will have a dataset called 'penguins.csv' which contains data of 344 cute penguins. In this dataset, there are 3 different species of penguins coming from 3 islands in the Palmer Archipelago. These three classes are Adelie, Chinstrap, and Gentoo. Also, these datasets contain culmen dimensions for each species. This dataset is created by Dr.Kristen Gorman and the Palmer Station, Antarctica LTER.\n",
    "\n",
    "If you want to have a deeper look at this dataset, search 'Palmer Penguin dataset' in your favourite search engine.\n",
    "\n",
    "The aim of our model is to be able to predict which species the penguin are (either Adelie, Chinstrap, and Gentoo) given some information (or formally called feature in the ML world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Import Relevant Libraries</b>\n",
    "\n",
    "So, let's get started. As always, the first step is always to import the relevant library first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                     \n",
    "import math\n",
    "from sklearn import svm             # Fetch a module called svm from sklearn library\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix         # Fetch a module called metrics from sklearn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Make a simple sklearn classifier</b>\n",
    "\n",
    "First, read the data in using pandas.read_csv(). Note that the final column contains the class_type field that we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>culmen_length_mm</th>\n",
       "      <th>culmen_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>MALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.3</td>\n",
       "      <td>20.6</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>MALE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  culmen_length_mm  culmen_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen              39.1             18.7              181.0   \n",
       "1  Adelie  Torgersen              39.5             17.4              186.0   \n",
       "2  Adelie  Torgersen              40.3             18.0              195.0   \n",
       "3  Adelie  Torgersen               NaN              NaN                NaN   \n",
       "4  Adelie  Torgersen              36.7             19.3              193.0   \n",
       "5  Adelie  Torgersen              39.3             20.6              190.0   \n",
       "\n",
       "   body_mass_g     sex  \n",
       "0       3750.0    MALE  \n",
       "1       3800.0  FEMALE  \n",
       "2       3250.0  FEMALE  \n",
       "3          NaN     NaN  \n",
       "4       3450.0  FEMALE  \n",
       "5       3650.0    MALE  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"penguins.csv\")               # Change the location of the file if it is not in the current directory\n",
    "data.head(6)                                # Take a look at the top 6 rows in the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data Preprocessing</b>\n",
    "\n",
    "For this step, we transform our data to a 'higher-quality' format. This comes from the idea that for a model to be accurate and precise in predictions is that the algorithm should be able to easily interpret the data's features. \n",
    "\n",
    "Some main step involved in this process is:\n",
    "- Data Cleaning\n",
    "- Data Transformation\n",
    "- Data Reduction (Dimensionality Reduction)\n",
    "\n",
    "We will look further into these steps during future lecture. If you want to look further, [this is a wonderful resource](https://developer.ibm.com/articles/data-preprocessing-in-detail/) is a which describes data preprocessing in a concise way.\n",
    "\n",
    "For now, we will just simply delete features that are non-numerical, which are 'island' and 'sex'. If you're wondering why can't we just change those features into numerical feature by classifying each entry in the feature by some value, we definitely can. But the content creator is just too lazy for this. Try it for yourself and see whether including 'island' and 'sex' feature does improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>culmen_length_mm</th>\n",
       "      <th>culmen_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species  culmen_length_mm  culmen_depth_mm  flipper_length_mm  body_mass_g\n",
       "0  Adelie              39.1             18.7              181.0       3750.0\n",
       "1  Adelie              39.5             17.4              186.0       3800.0\n",
       "2  Adelie              40.3             18.0              195.0       3250.0\n",
       "3  Adelie               NaN              NaN                NaN          NaN\n",
       "4  Adelie              36.7             19.3              193.0       3450.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_updated = data.drop(columns=['island', 'sex'])         # Drop 'island' and 'sex' features\n",
    "data_updated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all of our data is in the numerical format right? NO!!! If you look closely. There's a NaN value in our dataset. A simple workaround for this is just delete those entry which has NaN value. This is another aspect that you can try play to improve your model. Try changing the NaN value to the mean of each column, or maybe the mode of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>culmen_length_mm</th>\n",
       "      <th>culmen_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>39.3</td>\n",
       "      <td>20.6</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3650.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>47.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>214.0</td>\n",
       "      <td>4925.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>46.8</td>\n",
       "      <td>14.3</td>\n",
       "      <td>215.0</td>\n",
       "      <td>4850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>50.4</td>\n",
       "      <td>15.7</td>\n",
       "      <td>222.0</td>\n",
       "      <td>5750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>45.2</td>\n",
       "      <td>14.8</td>\n",
       "      <td>212.0</td>\n",
       "      <td>5200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>49.9</td>\n",
       "      <td>16.1</td>\n",
       "      <td>213.0</td>\n",
       "      <td>5400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>342 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    species  culmen_length_mm  culmen_depth_mm  flipper_length_mm  body_mass_g\n",
       "0    Adelie              39.1             18.7              181.0       3750.0\n",
       "1    Adelie              39.5             17.4              186.0       3800.0\n",
       "2    Adelie              40.3             18.0              195.0       3250.0\n",
       "4    Adelie              36.7             19.3              193.0       3450.0\n",
       "5    Adelie              39.3             20.6              190.0       3650.0\n",
       "..      ...               ...              ...                ...          ...\n",
       "338  Gentoo              47.2             13.7              214.0       4925.0\n",
       "340  Gentoo              46.8             14.3              215.0       4850.0\n",
       "341  Gentoo              50.4             15.7              222.0       5750.0\n",
       "342  Gentoo              45.2             14.8              212.0       5200.0\n",
       "343  Gentoo              49.9             16.1              213.0       5400.0\n",
       "\n",
       "[342 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_updated = data_updated.dropna()\n",
    "display(data_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to split our dataset into two categories, which are training set and testing set.\n",
    "\n",
    "First, let's assign all our features into a variable called all_X, while all our labels into a variable called all_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(342, 5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_updated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X = data_updated.iloc[:, 1:]\n",
    "all_y = data_updated.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_X, all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 4) (86, 4) (256,) (86,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Train and Evaluate the model</b>\n",
    "\n",
    "We just call the function `.fit()` to start the training of the model\n",
    "\n",
    "It's easy to swap in a different model of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Evaluate the model</b>\n",
    "\n",
    "To make a prediction, just call the fucntion `.predict()`\n",
    "Usually, we will use the unseen dataset (x_test) for our model to predict. This is because our main goal is we want to build a model that will be able to generalize well, meaning it can still perform well despite looking at dataset that it has never seen before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `.accuracy_score()` function to find the accuracy of our predicted dataset\n",
    "On top of that, `confusion_matrix()` will print a confusion matrix\n",
    "\n",
    "Click [here](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62) for further explanation on confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (using SVM):  0.7325581395348837\n",
      "[[35 18  1]\n",
      " [ 0  0  0]\n",
      " [ 3  1 28]]\n"
     ]
    }
   ],
   "source": [
    "scikit_accuracy = accuracy_score(predicted, y_test)\n",
    "print(\"Accuracy (using SVM): \", scikit_accuracy)\n",
    "print(confusion_matrix(predicted, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 Further Resources\n",
    "\n",
    "- [Scikit-Learn Tutorials](https://scikit-learn.org/stable/tutorial/index.html)\n",
    "- [Kaggle - Scikit-learn From Start to Finish](https://www.kaggle.com/code/jeffd23/scikit-learn-ml-from-start-to-finish)\n",
    "- [Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf)\n",
    "- [Documentation](https://scikit-learn.org/stable/modules/classes.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 TensorFlow\n",
    "\n",
    "TensorFlow is a library that makes machine learning and particularly developing neural networks faster and easier. It is mainly used for developing deep learning model. It makes building models easier, faster and more reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 Example\n",
    "\n",
    "For this example, we will build a simple neural network to solve the same problem, which is to predict the species of a penguin given some features.\n",
    "\n",
    "*Note that the exact same dataset (which has been preprocessed at earlier stage) will be used for the training of neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Import relevant libraries</b>\n",
    "\n",
    "As always, lets import the relevant module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data Preprocessing</b>\n",
    " \n",
    "In the Data Preprocessing step, there are a few things that we need to do:\n",
    "\n",
    "1. <b>Convert our dataset (which are in Pandas Series and DataFrame) into Numpy Array</b>\n",
    "- The reason for doing this is Tensorflow prefer to process its input in Numpy-array datatype, or even better in TensorFlow tensor datatype\n",
    "\n",
    "2. <b>Transform our label (which is currently in String type) into one-hot embedding </b>\n",
    "- This is attributed to the format of output of our neural network (which is it contains an array of 3 element)\n",
    "\n",
    "3. <b>Standardize our features</b>\n",
    "- This is primarily because of what happens inside of the NN (vanishing gradient problem can easily occured when the range of the features are very large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our dataset is still on Pandas Series and DataFrame type. We need to change it to Numpy array as Tensorflow (or rather, the function that we will call later) prefer to process its input in Numpy-array, or even better in TensorFlow tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical                       # to_categorical allows us to converts a class vector (integers) to binary class matrix.\n",
    "\n",
    "y_train = y_train.replace({'Adelie':0,'Chinstrap':1, 'Gentoo':2})       # Change the string in y_train into an integer\n",
    "y_test = y_test.replace({'Adelie':0,'Chinstrap':1, 'Gentoo':2})         # Change the string in y_test into an integer\n",
    "\n",
    "y_train = to_categorical(y_train)       # One-hot embedding\n",
    "y_test = to_categorical(y_test)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values            # Change X_train to numpy array\n",
    "X_test = X_test.values              # Change X_test from DataFrame to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  41.1,   18.6,  189. , 3325. ],\n",
       "       [  49.5,   19. ,  200. , 3800. ],\n",
       "       [  48.5,   17.5,  191. , 3400. ],\n",
       "       ...,\n",
       "       [  36.5,   18. ,  182. , 3150. ],\n",
       "       [  47.4,   14.6,  212. , 4725. ],\n",
       "       [  45.8,   14.2,  219. , 4700. ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To standardize our feature, the easy way is just to use the function provided by scikit-learn.\n",
    "\n",
    "What standardize does is that it subract our value by its mean and scale it to unit variance\n",
    "\n",
    "    z = (x - u) / s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.34978806, -0.11952499, -0.32646143, -0.86178483],\n",
       "       [ 1.2491795 , -1.55754071,  1.19114305,  1.88894803],\n",
       "       [-1.16195499,  1.10545136, -1.36119175, -1.11185146],\n",
       "       [-0.16688362,  0.67937263, -0.39544345,  0.07596501],\n",
       "       [ 0.5985559 , -1.55754071,  0.98419698,  0.82616488]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see that our input feature has been standardized. The value of each element is not wildly different as it is before the standardization operation is executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Build and Compile Model</b>\n",
    "\n",
    "First, read the data in using pandas.read_csv(). Note that the final column contains the class_type field that we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 32)                160       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               4224      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,347\n",
      "Trainable params: 29,347\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=32, activation='relu', input_dim=len(X_train[0])))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=3, activation='sigmoid'))\n",
    "\n",
    "model.summary()             # Print the summary of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.compile()` function used below is to compile our model. When we compile our model, what we're basically doing is telling TensorFlow how we want our model to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Fit, Predict and Evaluate</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training our neural network, there is one problem with our dataset, which is our label is in string format (meaning it is in words). And generally, neural network can't accept string format, hence we need to encode the value into a numerical format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to start our training by calling `.fit()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.1043 - accuracy: 0.1992\n",
      "Epoch 2/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.1027 - accuracy: 0.2031\n",
      "Epoch 3/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.1012 - accuracy: 0.2109\n",
      "Epoch 4/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0996 - accuracy: 0.2266\n",
      "Epoch 5/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0981 - accuracy: 0.2305\n",
      "Epoch 6/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0966 - accuracy: 0.2656\n",
      "Epoch 7/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0951 - accuracy: 0.2773\n",
      "Epoch 8/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0936 - accuracy: 0.2930\n",
      "Epoch 9/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0921 - accuracy: 0.3047\n",
      "Epoch 10/200\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.0906 - accuracy: 0.3242\n",
      "Epoch 11/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0891 - accuracy: 0.3398\n",
      "Epoch 12/200\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.0876 - accuracy: 0.3594\n",
      "Epoch 13/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0862 - accuracy: 0.3789\n",
      "Epoch 14/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0847 - accuracy: 0.3867\n",
      "Epoch 15/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0833 - accuracy: 0.4180\n",
      "Epoch 16/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0818 - accuracy: 0.4609\n",
      "Epoch 17/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0804 - accuracy: 0.4922\n",
      "Epoch 18/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0790 - accuracy: 0.5078\n",
      "Epoch 19/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0776 - accuracy: 0.5195\n",
      "Epoch 20/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0762 - accuracy: 0.5195\n",
      "Epoch 21/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0748 - accuracy: 0.5234\n",
      "Epoch 22/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0734 - accuracy: 0.5312\n",
      "Epoch 23/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0721 - accuracy: 0.5312\n",
      "Epoch 24/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0707 - accuracy: 0.5195\n",
      "Epoch 25/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0694 - accuracy: 0.5156\n",
      "Epoch 26/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0680 - accuracy: 0.5078\n",
      "Epoch 27/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0667 - accuracy: 0.5039\n",
      "Epoch 28/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0653 - accuracy: 0.5078\n",
      "Epoch 29/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0640 - accuracy: 0.5078\n",
      "Epoch 30/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0627 - accuracy: 0.4961\n",
      "Epoch 31/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0614 - accuracy: 0.4883\n",
      "Epoch 32/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0601 - accuracy: 0.4883\n",
      "Epoch 33/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0588 - accuracy: 0.4844\n",
      "Epoch 34/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0574 - accuracy: 0.4922\n",
      "Epoch 35/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0561 - accuracy: 0.5117\n",
      "Epoch 36/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0548 - accuracy: 0.5117\n",
      "Epoch 37/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0536 - accuracy: 0.5156\n",
      "Epoch 38/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0523 - accuracy: 0.5273\n",
      "Epoch 39/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0510 - accuracy: 0.5312\n",
      "Epoch 40/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0497 - accuracy: 0.5391\n",
      "Epoch 41/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0484 - accuracy: 0.5430\n",
      "Epoch 42/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0472 - accuracy: 0.5547\n",
      "Epoch 43/200\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.0459 - accuracy: 0.5664\n",
      "Epoch 44/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0446 - accuracy: 0.5586\n",
      "Epoch 45/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0434 - accuracy: 0.5625\n",
      "Epoch 46/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0421 - accuracy: 0.5703\n",
      "Epoch 47/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0409 - accuracy: 0.5820\n",
      "Epoch 48/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0396 - accuracy: 0.5938\n",
      "Epoch 49/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0383 - accuracy: 0.6016\n",
      "Epoch 50/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0371 - accuracy: 0.6055\n",
      "Epoch 51/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0358 - accuracy: 0.6055\n",
      "Epoch 52/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0345 - accuracy: 0.6055\n",
      "Epoch 53/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0333 - accuracy: 0.6094\n",
      "Epoch 54/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0320 - accuracy: 0.6211\n",
      "Epoch 55/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0308 - accuracy: 0.6211\n",
      "Epoch 56/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0295 - accuracy: 0.6250\n",
      "Epoch 57/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0283 - accuracy: 0.6289\n",
      "Epoch 58/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0270 - accuracy: 0.6289\n",
      "Epoch 59/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0258 - accuracy: 0.6289\n",
      "Epoch 60/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0245 - accuracy: 0.6367\n",
      "Epoch 61/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0233 - accuracy: 0.6406\n",
      "Epoch 62/200\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.0221 - accuracy: 0.6445\n",
      "Epoch 63/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0208 - accuracy: 0.6445\n",
      "Epoch 64/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0196 - accuracy: 0.6445\n",
      "Epoch 65/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0184 - accuracy: 0.6484\n",
      "Epoch 66/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0171 - accuracy: 0.6523\n",
      "Epoch 67/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0159 - accuracy: 0.6562\n",
      "Epoch 68/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0147 - accuracy: 0.6562\n",
      "Epoch 69/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0135 - accuracy: 0.6562\n",
      "Epoch 70/200\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.0123 - accuracy: 0.6562\n",
      "Epoch 71/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0110 - accuracy: 0.6641\n",
      "Epoch 72/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0098 - accuracy: 0.6641\n",
      "Epoch 73/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0086 - accuracy: 0.6758\n",
      "Epoch 74/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.0074 - accuracy: 0.6797\n",
      "Epoch 75/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0062 - accuracy: 0.6797\n",
      "Epoch 76/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0050 - accuracy: 0.6797\n",
      "Epoch 77/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0038 - accuracy: 0.6797\n",
      "Epoch 78/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0025 - accuracy: 0.6797\n",
      "Epoch 79/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 1.0013 - accuracy: 0.6836\n",
      "Epoch 80/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.0001 - accuracy: 0.6797\n",
      "Epoch 81/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9989 - accuracy: 0.6836\n",
      "Epoch 82/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9977 - accuracy: 0.6836\n",
      "Epoch 83/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9965 - accuracy: 0.6836\n",
      "Epoch 84/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9952 - accuracy: 0.6875\n",
      "Epoch 85/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9940 - accuracy: 0.6875\n",
      "Epoch 86/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9928 - accuracy: 0.6875\n",
      "Epoch 87/200\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.9916 - accuracy: 0.6875\n",
      "Epoch 88/200\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.9904 - accuracy: 0.6875\n",
      "Epoch 89/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9892 - accuracy: 0.6875\n",
      "Epoch 90/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9879 - accuracy: 0.6875\n",
      "Epoch 91/200\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.9867 - accuracy: 0.6875\n",
      "Epoch 92/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9855 - accuracy: 0.6875\n",
      "Epoch 93/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9843 - accuracy: 0.6875\n",
      "Epoch 94/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9830 - accuracy: 0.6875\n",
      "Epoch 95/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9818 - accuracy: 0.6875\n",
      "Epoch 96/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9806 - accuracy: 0.6875\n",
      "Epoch 97/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9794 - accuracy: 0.6875\n",
      "Epoch 98/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9781 - accuracy: 0.6875\n",
      "Epoch 99/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9769 - accuracy: 0.6875\n",
      "Epoch 100/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9757 - accuracy: 0.6953\n",
      "Epoch 101/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9745 - accuracy: 0.6953\n",
      "Epoch 102/200\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.9732 - accuracy: 0.6953\n",
      "Epoch 103/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9720 - accuracy: 0.6992\n",
      "Epoch 104/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9708 - accuracy: 0.6992\n",
      "Epoch 105/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9696 - accuracy: 0.6992\n",
      "Epoch 106/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9683 - accuracy: 0.6992\n",
      "Epoch 107/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9671 - accuracy: 0.6992\n",
      "Epoch 108/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9659 - accuracy: 0.7031\n",
      "Epoch 109/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9646 - accuracy: 0.7031\n",
      "Epoch 110/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9634 - accuracy: 0.7070\n",
      "Epoch 111/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9622 - accuracy: 0.7070\n",
      "Epoch 112/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9610 - accuracy: 0.7109\n",
      "Epoch 113/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9598 - accuracy: 0.7109\n",
      "Epoch 114/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9585 - accuracy: 0.7109\n",
      "Epoch 115/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9573 - accuracy: 0.7148\n",
      "Epoch 116/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9561 - accuracy: 0.7148\n",
      "Epoch 117/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9549 - accuracy: 0.7148\n",
      "Epoch 118/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9537 - accuracy: 0.7188\n",
      "Epoch 119/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9524 - accuracy: 0.7188\n",
      "Epoch 120/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9512 - accuracy: 0.7266\n",
      "Epoch 121/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9500 - accuracy: 0.7266\n",
      "Epoch 122/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9488 - accuracy: 0.7266\n",
      "Epoch 123/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9475 - accuracy: 0.7266\n",
      "Epoch 124/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9463 - accuracy: 0.7266\n",
      "Epoch 125/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9451 - accuracy: 0.7266\n",
      "Epoch 126/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9439 - accuracy: 0.7266\n",
      "Epoch 127/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9426 - accuracy: 0.7266\n",
      "Epoch 128/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9414 - accuracy: 0.7344\n",
      "Epoch 129/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9402 - accuracy: 0.7344\n",
      "Epoch 130/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9389 - accuracy: 0.7383\n",
      "Epoch 131/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9377 - accuracy: 0.7383\n",
      "Epoch 132/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9365 - accuracy: 0.7383\n",
      "Epoch 133/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9353 - accuracy: 0.7383\n",
      "Epoch 134/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9340 - accuracy: 0.7383\n",
      "Epoch 135/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9328 - accuracy: 0.7383\n",
      "Epoch 136/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9316 - accuracy: 0.7383\n",
      "Epoch 137/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9303 - accuracy: 0.7383\n",
      "Epoch 138/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9291 - accuracy: 0.7461\n",
      "Epoch 139/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9279 - accuracy: 0.7461\n",
      "Epoch 140/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9267 - accuracy: 0.7461\n",
      "Epoch 141/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9254 - accuracy: 0.7461\n",
      "Epoch 142/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.9242 - accuracy: 0.7461\n",
      "Epoch 143/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9230 - accuracy: 0.7461\n",
      "Epoch 144/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9217 - accuracy: 0.7461\n",
      "Epoch 145/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9205 - accuracy: 0.7461\n",
      "Epoch 146/200\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.9193 - accuracy: 0.7461\n",
      "Epoch 147/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9180 - accuracy: 0.7461\n",
      "Epoch 148/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9168 - accuracy: 0.7461\n",
      "Epoch 149/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9156 - accuracy: 0.7500\n",
      "Epoch 150/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9143 - accuracy: 0.7500\n",
      "Epoch 151/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9131 - accuracy: 0.7539\n",
      "Epoch 152/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9118 - accuracy: 0.7539\n",
      "Epoch 153/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9106 - accuracy: 0.7539\n",
      "Epoch 154/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9094 - accuracy: 0.7578\n",
      "Epoch 155/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9081 - accuracy: 0.7578\n",
      "Epoch 156/200\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.9069 - accuracy: 0.7578\n",
      "Epoch 157/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9056 - accuracy: 0.7578\n",
      "Epoch 158/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.9044 - accuracy: 0.7578\n",
      "Epoch 159/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9032 - accuracy: 0.7578\n",
      "Epoch 160/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9020 - accuracy: 0.7578\n",
      "Epoch 161/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9007 - accuracy: 0.7578\n",
      "Epoch 162/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8994 - accuracy: 0.7578\n",
      "Epoch 163/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8982 - accuracy: 0.7617\n",
      "Epoch 164/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8970 - accuracy: 0.7617\n",
      "Epoch 165/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8957 - accuracy: 0.7617\n",
      "Epoch 166/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8945 - accuracy: 0.7656\n",
      "Epoch 167/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8932 - accuracy: 0.7695\n",
      "Epoch 168/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8920 - accuracy: 0.7695\n",
      "Epoch 169/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8908 - accuracy: 0.7734\n",
      "Epoch 170/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8895 - accuracy: 0.7734\n",
      "Epoch 171/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8883 - accuracy: 0.7734\n",
      "Epoch 172/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8870 - accuracy: 0.7734\n",
      "Epoch 173/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8858 - accuracy: 0.7734\n",
      "Epoch 174/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8845 - accuracy: 0.7734\n",
      "Epoch 175/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8833 - accuracy: 0.7734\n",
      "Epoch 176/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8820 - accuracy: 0.7734\n",
      "Epoch 177/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8808 - accuracy: 0.7734\n",
      "Epoch 178/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8795 - accuracy: 0.7734\n",
      "Epoch 179/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8783 - accuracy: 0.7734\n",
      "Epoch 180/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8770 - accuracy: 0.7734\n",
      "Epoch 181/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8758 - accuracy: 0.7734\n",
      "Epoch 182/200\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.8746 - accuracy: 0.7734\n",
      "Epoch 183/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8733 - accuracy: 0.7734\n",
      "Epoch 184/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8721 - accuracy: 0.7734\n",
      "Epoch 185/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8708 - accuracy: 0.7734\n",
      "Epoch 186/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8696 - accuracy: 0.7734\n",
      "Epoch 187/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8683 - accuracy: 0.7734\n",
      "Epoch 188/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8671 - accuracy: 0.7734\n",
      "Epoch 189/200\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8658 - accuracy: 0.7734\n",
      "Epoch 190/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8646 - accuracy: 0.7734\n",
      "Epoch 191/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8633 - accuracy: 0.7734\n",
      "Epoch 192/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8621 - accuracy: 0.7734\n",
      "Epoch 193/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8608 - accuracy: 0.7734\n",
      "Epoch 194/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8596 - accuracy: 0.7734\n",
      "Epoch 195/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8583 - accuracy: 0.7734\n",
      "Epoch 196/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8571 - accuracy: 0.7734\n",
      "Epoch 197/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8558 - accuracy: 0.7734\n",
      "Epoch 198/200\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8546 - accuracy: 0.7773\n",
      "Epoch 199/200\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.8533 - accuracy: 0.7773\n",
      "Epoch 200/200\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8520 - accuracy: 0.7773\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=200, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have trained it, we can see how our model perform on each of the training epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAsRklEQVR4nO3dd3hUddrG8e+TRiihF6lSRCR0CAhSXYoUAUFFwN4QFaW4u7Z3V7e57qooHUGxsBZsKCpKU0IvoXfpEkCqNOnwe/+Yg0acYAKZnElyf64rF5NTZp6cDHPnnN855zHnHCIiIueL8LsAEREJTwoIEREJSgEhIiJBKSBERCQoBYSIiASlgBARkaAUECIZwMzeNLN/pnHZLWbW6lKfRyTUFBAiIhKUAkJERIJSQEiO4R3a+ZOZLTezn8zsdTMrYWZfmdlhM5tqZoVSLN/JzFaZ2QEzm25mVVPMq2Nmi731xgGx573W9Wa21Ft3jpnVvMia7zezDWa238wmmFkpb7qZ2ctmttvMDno/U3VvXnszW+3Vtt3M/nhRG0xyPAWE5DQ3Aq2BK4GOwFfAU0BRAv8fHgUwsyuB94B+QDFgIvC5mcWYWQzwKTAWKAx86D0v3rp1gTHAA0AR4FVggpnlSk+hZvYH4N9AN6AksBV435vdBmjm/RwFgVuAfd6814EHnHNxQHXgm/S8rsg5CgjJaYY453Y557YDM4H5zrklzrkTwHigjrfcLcCXzrkpzrlTwItAbuAaoCEQDbzinDvlnPsIWJjiNe4HXnXOzXfOnXHOvQWc8NZLj1uBMc65xV59TwKNzKw8cAqIA64CzDm3xjm301vvFBBvZvmdcz865xan83VFAAWE5Dy7Ujw+FuT7fN7jUgT+YgfAOXcW2AaU9uZtd7++0+XWFI8vBx7zDi8dMLMDQFlvvfQ4v4YjBPYSSjvnvgGGAsOAXWY2yszye4veCLQHtppZopk1SufrigAKCJHU7CDwQQ8EjvkT+JDfDuwESnvTzimX4vE24F/OuYIpvvI45967xBryEjhktR3AOTfYOVcPqEbgUNOfvOkLnXOdgeIEDoV9kM7XFQEUECKp+QDoYGYtzSwaeIzAYaI5wFzgNPComUWZWVegQYp1RwO9zexqbzA5r5l1MLO4dNbwLnC3mdX2xi+eI3BIbIuZ1feePxr4CTgOnPHGSG41swLeobFDwJlL2A6SgykgRIJwzq0DbgOGAHsJDGh3dM6ddM6dBLoCdwE/Ehiv+CTFukkExiGGevM3eMumt4ZpwF+AjwnstVQCunuz8xMIoh8JHIbaR2CcBOB2YIuZHQJ6ez+HSLqZGgaJiEgw2oMQEZGgFBAiIhKUAkJERIJSQIiISFBRfheQkYoWLerKly/vdxkiIlnGokWL9jrnigWbl60Conz58iQlJfldhohIlmFmW1Obp0NMIiISlAJCRESCUkCIiEhQ2WoMQkQkvU6dOkVycjLHjx/3u5SQio2NpUyZMkRHR6d5HQWEiORoycnJxMXFUb58eX59g97swznHvn37SE5OpkKFCmleT4eYRCRHO378OEWKFMm24QBgZhQpUiTde0kKCBHJ8bJzOJxzMT+jAgIYMm09K7cf9LsMEZGwErKAMLMxZrbbzFamMv8qM5trZifM7I/nzWtrZuvMbIOZPRGqGgF+/Okk7y74nhtHzOHjRcmhfCkRkd84cOAAw4cPT/d67du358CBAxlfUAqh3IN4E2h7gfn7gUf5pckJAGYWSaDPbjsgHuhhZvEhqpFCeWP4/JEm1C1XiMc+XMZfP1vJydNnQ/VyIiK/klpAnDlz4UaAEydOpGDBgiGqKiBkAeGcm0EgBFKbv9s5txA4dd6sBsAG59wmr3PX+0DnUNUJUDRfLsbe24BezSry9tyt9Bg9j12HsvcpbyISHp544gk2btxI7dq1qV+/Ptdeey09e/akRo0aANxwww3Uq1ePatWqMWrUqJ/XK1++PHv37mXLli1UrVqV+++/n2rVqtGmTRuOHTuWIbWF42mupQk0fT8nGbg61C8aFRnBU+2rUrNMAf780XKuHzKL4bfWpX75wqF+aREJE3/7fBWrdxzK0OeML5WfZzpWS3X+888/z8qVK1m6dCnTp0+nQ4cOrFy58ufTUceMGUPhwoU5duwY9evX58Ybb6RIkSK/eo7169fz3nvvMXr0aLp168bHH3/MbbddeqfZcBykDjbUnmpfVDPrZWZJZpa0Z8+eS37x62uWYvxDjcmXK4oeo+bx5uzNqC2riGSWBg0a/OpahcGDB1OrVi0aNmzItm3bWL9+/W/WqVChArVr1wagXr16bNmyJUNqCcc9iGSgbIrvywA7UlvYOTcKGAWQkJCQIZ/kVS6L47M+jRkwbhnPfr6aZckHea5LDXLHRGbE04tImLrQX/qZJW/evD8/nj59OlOnTmXu3LnkyZOHFi1aBL2WIVeuXD8/joyMzLBDTOG4B7EQqGxmFcwsBugOTMjsIvLHRjPq9no81vpKPl26na4j5vD9vqOZXYaIZHNxcXEcPnw46LyDBw9SqFAh8uTJw9q1a5k3b16m1hayPQgzew9oARQ1s2TgGSAawDk30swuA5KA/MBZM+sHxDvnDplZH2ASEAmMcc6tClWdFxIRYTzSsjLVyxSg3/tLuX7ITF7qVpvW8SX8KEdEsqEiRYrQuHFjqlevTu7cuSlR4pfPl7Zt2zJy5Ehq1qxJlSpVaNiwYabWZtnp+HpCQoILVcOgbfuP8tA7i1mx/SAPNKvIH6+rQnRkOO6AiUh6rFmzhqpVq/pdRqYI9rOa2SLnXEKw5fUJl0ZlC+fhw96NuL3h5bw6YxM9R8/jh4M6FVZEsi8FRDrERkfyjxuqM6h7bVbtOESHwTOZtX6v32WJiISEAuIidK5dmgl9mlAkXwy3j5nPK1O/48zZ7HOoTiSnyU6H2lNzMT+jAuIiXVE8H58+3JgudUrzytT13PXGAvYdOeF3WSKSTrGxsezbty9bh8S5fhCxsbHpWk+D1JfIOce4hdv464RVFM4Tw9CedUjQ1dciWUZO7yh3oUFqBUQGWbXjIA+9s5jkH48xoPWV9G5eiciI7H+PeRHJ2nQWUyaoVqoAXzzShA41SvLCpHXc/vp83fBPRLI0BUQGiouNZlD32vz3ppos+f4A7QbN5Ju1u/wuS0TkoiggMpiZ0S2hLJ8/0oQS+WO5580k/v75ak6cvvC93UVEwo0CIkSuKJ6P8Q9dw13XlGfM7M10HT6HTXuO+F2WiEiaKSBCKDY6kmc7VeO1OxLYceAY1w+ZxUeLkrP16XQikn0oIDJBq/gSfNW3GTXLFOCPHy6j/7ilHD5+fiM9EZHwooDIJJcViOWd+xryWOsr+Xz5Tq4fMoul2w74XZaISKoUEJko0rt9+LheDTl9xnHjiDkMmrqe02fO+l2aiMhvKCB8kFC+MBP7NqVjzZK8PPU7bho5l817f/K7LBGRX1FA+KRA7mhe6V6HIT3qsHnvT7QfNJN35m/VALaIhA0FhM861irFpH7NqHd5IZ4ev5L73kpiz2Hd9E9E/KeACAOXFYjl7Xsa8EzHeGZt2Mt1r8xg8qof/C5LRHI4BUSYiIgw7m5cgS8eaULJArH0GruIxz9azpETp/0uTURyKAVEmKlcIo7xDzXmoRaV+GDRNtoPmsmirfv9LktEciAFRBiKiYrgz22v4oMHGnHWOW4eOZfnv1rL8VO6n5OIZB4FRBirX74wX/Vtys31yjIycSMdh8ximS6uE5FMooAIc3Gx0fznppq8eXd9Dh8/TdcRc3hh0lrdHVZEQk4BkUW0qFKcSf2b0bVOaYZ9u5FOQ2azIvmg32WJSDamgMhCCuSO5oWba/HGXfU5cOwkNwyfzUuT13HytG7VISIZTwGRBV17VXEm92tO59qlGPLNBjoNncXK7dqbEJGMpYDIogrkiWZgt9q8dkcC+346yQ3DZvPylO+0NyEiGUYBkcW1ii/BlP7N6FirFIOmreeGYbNZveOQ32WJSDaggMgGCuaJ4eVbavPq7fXYffg4nYbOYvC09ZzSbcRF5BIoILKR66pdxuT+zWlXoyQDp3xHl+GzWfuD9iZE5OIoILKZwnljGNKjDiNurcvOA8fpOGQWQ79RUyIRSb+QBYSZjTGz3Wa2MpX5ZmaDzWyDmS03s7op5m0xsxVmttTMkkJVY3bWrkZJJvdvRptql/Hi5O/oMnyO9iZEJF1CuQfxJtD2AvPbAZW9r17AiPPmX+ucq+2cSwhNedlfkXy5GNazLsN61mXHgWNcP3gWA6d8p6uwRSRNQhYQzrkZwIVuQ9oZeNsFzAMKmlnJUNWTk3WoWZIpA5pzfc2SDJ62no5DZrHk+x/9LktEwpyfYxClgW0pvk/2pgE4YLKZLTKzXhd6EjPrZWZJZpa0Z8+eEJWa9RXOG8Mr3esw5q4EDh8/zY0j5vDPL1Zz7KT2JkQkOD8DwoJMO9eQubFzri6Bw1APm1mz1J7EOTfKOZfgnEsoVqxYKOrMVv5wVQkm929GjwbleG3WZq57ZQZzNu71uywRCUN+BkQyUDbF92WAHQDOuXP/7gbGAw0yvbpsLC42mn91qcF79zfEDHqOns+Tnyzn0PFTfpcmImHEz4CYANzhnc3UEDjonNtpZnnNLA7AzPICbYCgZ0LJpWlUqQhf921Gr2YVGbdwG60HJjJ19S6/yxKRMBHK01zfA+YCVcws2czuNbPeZtbbW2QisAnYAIwGHvKmlwBmmdkyYAHwpXPu61DVmdPljonkqfZVGf9QYwrlieG+t5N49L0l7Dtywu/SRMRn5pz7/aWyiISEBJeUpMsmLtbJ02cZMX0jQ79dT1xsNM90jKdTrVKYBRsuEpHswMwWpXY5ga6klp/FREXQt1VlvnikKWUL56Hv+0u5760kfjh43O/SRMQHCgj5jSqXxfHJg9fwfx2qMnvjXloPTOSd+Vs5ezb77G2KyO9TQEhQkRHGfU0rMqlfM2qUKcDT41fSffQ8Nu454ndpIpJJFBByQZcXycs7913Nf2+sydqdh2g3aCZDv1mvxkQiOYACQn6XmdGtflmmPtac1lVL8OLk7+g0dBZLtx3wuzQRCSEFhKRZ8bhYht1al1G31+PA0VN0GT6bv3++mp9OnPa7NBEJAQWEpFubapcxZUAzbrv6csbM3kybl2cwfd1uv8sSkQymgJCLEhcbzT9uqM6HvRsRGx3BXW8spP+4pez/6aTfpYlIBlFAyCWpX74wE/s25dGWlfli+Q5aDUzk0yXbyU4XYIrkVAoIuWS5oiIZ0PpKvnikKeUK56HfuKXc+cZCtu0/6ndpInIJFBCSYapcFsfHD17Dsx3jSdqynzYvz+D1WZs5owvsRLIkBYRkqMgI467GFZgyoDkNKxbmH1+spuvw2azZqX7YIlmNAkJConTB3Iy5qz6Dutcm+cdjdBwyixcnreP4KXWwE8kqFBASMmZG59qlmTqgOZ1rl2botxtoP2gm8zft87s0EUkDBYSEXKG8MbzUrRZj723AyTNnuWXUPJ4av0Id7ETCnAJCMk3TysWY3L8Z9zetwPsLvqfVS4l8vfIHv8sSkVQoICRT5YmJ4ukO8Xz6cGOK5MtF7/8totfbSew8eMzv0kTkPAoI8UXNMgWZ0Kcxj7e9ihnr99DqpUTemK1TYkXCiQJCfBMdGcGDLSoxuV9z6pUvzN8+X02X4bNZuf2g36WJCAoICQPliuThrbvrM6RHHXYcOE6nobP4xxe6S6yI3xQQEhbMjI61SjHtseb0aFCO12dtpvXARKas3uV3aSI5lgJCwkqB3NH8q0sNPn6wEXGx0dz/dhIPjNUgtogfFBASlupdXpgvHm3Cn9tWYfo6DWKL+EEBIWErOjKCh1pcwZT+GsQW8YMCQsLeuUHswSkGsf+pQWyRkFNASJZgZnSqVYppA5rTvUE5XvMGsadqEFskZBQQkqUUyBPNc11q8FHvRuSLjeK+t5PoPXYRPxw87ndpItmOAkKypITyhfnikab86boqfLtuN60GJvLWnC0axBbJQAoIybJioiJ4+NormNy/GXXKFeSZCavoOnw2q3ZoEFskIyggJMu7vEhe3r6nAYO612b7gWN0Gjqbf325mqMnNYgtcilCFhBmNsbMdpvZylTmm5kNNrMNZrbczOqmmNfWzNZ5854IVY2SfaRsTtQtoQyjZ26m9cAZfLNWg9giFyuUexBvAm0vML8dUNn76gWMADCzSGCYNz8e6GFm8SGsU7KRgnli+HfXmnzYuxF5YiK5580kHn5nMbsOaRBbJL1CFhDOuRnA/gss0hl42wXMAwqaWUmgAbDBObfJOXcSeN9bViTN6pcvzJePNuWPba5kyppdtHopkbFzNYgtkh5+jkGUBral+D7Zm5ba9KDMrJeZJZlZ0p49e0JSqGRNMVER9PlDZSb1a0bNsgX4y2eruHHEHNbsPOR3aSJZgp8BYUGmuQtMD8o5N8o5l+CcSyhWrFiGFSfZR4WiefnfvVfz8i21+H7/Ua4fMot/f7VGg9giv8PPgEgGyqb4vgyw4wLTRS6amdGlThmmDWjOjXVL82riJtq8PIPp63b7XZpI2PIzICYAd3hnMzUEDjrndgILgcpmVsHMYoDu3rIil6xQ3hj+e1Mt3u/VkFxREdz1xkL6vLuY3Yc1iC1yvqhQPbGZvQe0AIqaWTLwDBAN4JwbCUwE2gMbgKPA3d6802bWB5gERAJjnHOrQlWn5EwNKxZhYt+mvJq4iaHfbCDxuz080e4qetQvR0REsKOcIjmPOZd9zupISEhwSUlJfpchWcymPUd4evxK5m7aR91yBfl315pUuSzO77JEMoWZLXLOJQSbpyupJcerWCwf795/NS/eXIvNe3+iw+CZ/OfrtRw7ecbv0kR8pYAQITCIfVO9Mkx7rAU31CnNiOkbue6VGcz4TqdOS86lgBBJoXDeGF68uRbv3n81URHGHWMW0Pf9Jew5fMLv0kQynQJCJIhrKhVlYt+m9G1Zma9W/EDLl6bz3oLvOasrsSUHUUCIpCI2OpL+ra9kYt+mVC2Znyc/WcEto+ayftdhv0sTyRRpCggz62tm+b1rFl43s8Vm1ibUxYmEgyuK5+P9Xg357001Wb/7CO0Hz+TFSes4fkqD2JK9pXUP4h7n3CGgDVCMwDULz4esKpEwY2Z0SyjLtAHN6VizFEO/3UDbV2Ywa/1ev0sTCZm0BsS5K4faA28455YR/J5JItlakXy5GHhLbd6572oAbnt9Pv3HLWXfEQ1iS/aT1oBYZGaTCQTEJDOLA86GriyR8Nb4iqJ83a8Zj/7hCr5YvoOWAxP5YOE2stOFpyJpupLazCKA2sAm59wBMysMlHHOLQ9xfemiK6nFD+t3Heap8StYuOVHGlQozHNdqnNFcV2JLVlDRlxJ3QhY54XDbcD/AeoMLwJULhHHuF6NeL5rDdbuPES7QTMZOFmD2JL1pTUgRgBHzawW8GdgK/B2yKoSyWIiIozuDcox7bEWdKhRksHfbKDdoJnM2aBBbMm60hoQp13gWFRnYJBzbhCgfWiR8xSLy8Ur3esw9t4GnHWOnq/NZ8AHS9n/00m/SxNJt7QGxGEzexK4HfjSzCLxbt0tIr/VtHIxJvVrxsPXVmLC0h20fGk6HyZpEFuylrQGxC3ACQLXQ/xAoEf0CyGrSiQbiI2O5E/XXcXEvk2pWCwff/poOT1Gz2PjniN+lyaSJmnuB2FmJYD63rcLnHNh16tRZzFJuDp71vH+wm38+6s1nDh1loeurcSDLSqRKyrS79Ikh7vks5jMrBuwALgZ6AbMN7ObMq5EkewtIsLoeXU5pj3WnOuqX8YrU9fTbtBM5m7c53dpIqlK63UQy4DW5/YazKwYMNU5VyvE9aWL9iAkq5i+bjd/+Wwl2/Yf4+Z6ZXiqfVUK5Y3xuyzJgTLiOoiI8w4p7UvHuiJynhZVijO5X3MebFGJ8Uu203JgIp8sTtYgtoSVtH7If21mk8zsLjO7C/gSmBi6skSyv9wxkTze9iq+eLQJlxfJw4APlnHra/PZvPcnv0sTAdI3SH0j0JjATfpmOOfGh7Kwi6FDTJJVnT3reHfB9/zn67WcOH2WPtdewQPNK2oQW0LuQoeY0hwQWYECQrK63YeO87cvVvPl8p1cUTwfz3WpQYMKhf0uS7Kxix6DMLPDZnYoyNdhMzsUmnJFcq7i+WMZ1rMub9xVn2Mnz9Dt1bk8/tFyDhzVldiS+S4YEM65OOdc/iBfcc65/JlVpEhOc+1VxZkyoBkPNKvIR4uTaflSIp8u2a5BbMlUOhNJJEzliYniyfZV+bxPE8oUzkO/cUu5Y8wCtu7TILZkDgWESJiLL5WfTx68hr93rsaS7w/Q5uUZDPt2AydPq2eXhJYCQiQLiIww7mhUnmmPNecPVxXnhUnr6DB4Jgu37Pe7NMnGFBAiWUiJ/LGMuK0er92RwNGTZ7h55Fye/GQFB4+e8rs0yYYUECJZUKv4Ekzu34z7m1Zg3MLvaTkwkQnLdmgQWzKUAkIki8qbK4qnO8QzoU8TShWM5dH3lnDnGwv5ft9Rv0uTbCKkAWFmbc1snZltMLMngswvZGbjzWy5mS0ws+op5m0xsxVmttTMdPWbSCqqly7A+Ica82zHeBZt2U/rlxMZPn0Dp85oEFsuTcgCwus6NwxoB8QDPcws/rzFngKWOudqAncAg86bf61zrnZqV/mJSEBkhHFX4wpMfaw5LaoU479fr+P6wbNYtPVHv0uTLCyUexANgA3OuU3OuZPA+wR6WqcUD0wDcM6tBcp7jYlE5CKULJCbV29PYNTt9Th0/BQ3jZzD0+NXcPCYBrEl/UIZEKWBbSm+T/ampbQM6ApgZg2Ay4Ey3jwHTDazRWbWK7UXMbNeZpZkZkl79uzJsOJFsrI21S5jyoDm3H1NBd5b8D2tBibyxXINYkv6hDIgLMi089+dzwOFzGwp8AiwBDjtzWvsnKtL4BDVw2bWLNiLOOdGOecSnHMJxYoVy5jKRbKBfLmi+GvHeD57uAkl8ueiz7tLuOfNhWzbr0FsSZtQBkQyUDbF92WAHSkXcM4dcs7d7ZyrTWAMohiw2Zu3w/t3NzCewCErEUmnGmUK8OlDjfnL9fHM3xwYxH41caMGseV3hTIgFgKVzayCmcUA3YEJKRcws4LePID7CPSZOGRmec0szlsmL9AGWBnCWkWytajICO5tUoGpA5rT5Ipi/PurtXQcMosl32sQW1IXsoBwzp0G+gCTgDXAB865VWbW28x6e4tVBVaZ2VoCh5L6etNLALO8XtgLgC+dc1+HqlaRnKJUwdy8dmcCI2+rx4Gjp+g6Yg5//Wwlh45rEFt+Sw2DRHKow8dP8dLk73hr7haKx+Xi2Y7VaFv9MsyCDR9KdnXRDYNEJPuKi43m2U7V+PShxhTJm4sH31nMfW8lkfyjBrElQAEhksPVKluQCX0a838dqjJn4z5aD5zBazM3cVqD2DmeAkJEiIqM4L6mFZkyoBmNKhXhn1+uocvwOazcftDv0sRHCggR+VmZQnl4/c4Ehvasw86Dx+k8bDbPTVzD0ZOnf39lyXYUECLyK2bG9TVLMW1Ac7ollGHUjE20eXkGid/pTgU5jQJCRIIqkCeaf3etybheDYmJiuDOMQvo+/4S9h454XdpkkkUECJyQVdXLMJXfZvSt2VlJq7YSauBiXyQtE33dcoBFBAi8rtyRUXSv/WVTHy0KZWL5+PPHy2n5+j5bN77k9+lSQgpIEQkzSqXiGNcr0Y816UGK3cc5LpXZjDs2w2cPK1TYrMjBYSIpEtEhNHz6nJMG9CcVlWL88KkdXQcMovFuq9TtqOAEJGLUjx/LMNvrcfoOxI4dPwUN3r3dTqs+zplGwoIEbkkreNLMGVAc+5sVJ6x87bSeuAMJq36we+yJAMoIETkkuXLFcWznaox/qHGFMwTzQNjF/HA2CR+OHjc79LkEiggRCTD1C5bkM8facLjba9i+ro9tB6YyNh5Wzl7VqfEZkUKCBHJUNGRETzYohKT+zejVtmC/OXTldz86ly+23XY79IknRQQIhISlxfJy9h7GzCwWy027TlCh8EzeWnyOo6fOuN3aZJGCggRCRkzo2vdMkwd0JyONUsx5JsNtBs0k7kb9/ldmqSBAkJEQq5IvlwMvKU2Y+9twJmzjh6j5/H4R8s5cPSk36XJBSggRCTTNK1cjEn9mtG7eSU+WpxMq4GJTFi2Q/d1ClMKCBHJVLljInmi3VV83qcJpQvm5tH3lnD3mwvZtl+tTsONAkJEfBFfKj+fPNSYv14fz4LN+2nzslqdhhsFhIj4JjLCuKdJBaYMaM41XqvTG4bPVqvTMKGAEBHflS6Ym9fuTGBYz7r8cPAEnYfN5l9frlarU58pIEQkLJgZHWqW9FqdlmX0zM20HjiD6et2+11ajqWAEJGwEmh1WoMPHmhEbHQEd72xUK1OfaKAEJGw1KBCYSb2bUq/VpX5asUPtHxJrU4zmwJCRMJWrqhI+rW6kol9m3BliV9anW7ac8Tv0nIEBYSIhL0riv+61WnbQTMZ+s16tToNMQWEiGQJKVudtq5aghcnf0fHIbNYtFWtTkNFASEiWUrx/LEMu7Uur92RwOHjp7hppFqdhkpIA8LM2prZOjPbYGZPBJlfyMzGm9lyM1tgZtXTuq6I5Gyt4ksweUBz7rpGrU5DJWQBYWaRwDCgHRAP9DCz+PMWewpY6pyrCdwBDErHuiKSw+XLFcUzHdXqNFRCuQfRANjgnNvknDsJvA90Pm+ZeGAagHNuLVDezEqkcV0REeC3rU5bDUxk7NwtanV6iUIZEKWBbSm+T/ampbQM6ApgZg2Ay4EyaVwXb71eZpZkZkl79uzJoNJFJKtJ2eq0dtmC/OWzVdw0cg7rflCr04sVyoCwINPOj/PngUJmthR4BFgCnE7juoGJzo1yziU45xKKFSt2CeWKSHaQstXp5r0/0WHwTF6cpFanFyMqhM+dDJRN8X0ZYEfKBZxzh4C7AczMgM3eV57fW1dEJDXnWp22qFKcf365mqHfbuDLFTt5rksNGlUq4nd5WUYo9yAWApXNrIKZxQDdgQkpFzCzgt48gPuAGV5o/O66IiK/p3DeGAZ2q83/7r3651anf/5omVqdplHIAsI5dxroA0wC1gAfOOdWmVlvM+vtLVYVWGVmawmcsdT3QuuGqlYRyd6aVC7KpH7NeLBFJT5evJ1WAxP5bOl23dfpd1h22kAJCQkuKSnJ7zJEJIyt3nGIJz9ZzrLkgzS/shj/vKE6ZQvn8bss35jZIudcQrB5upJaRHKUc61On+kYz8ItgVano2eo1WkwCggRyXEiI4y7G//S6vRfE9XqNBgFhIjkWOdanQ6/tS67Dp2g09BZanWaggJCRHI0M6N9jZJMHdCc7g3KMXrmZtq8PIPE73ThrQJCRAQokDua57oEWp3miorgzjEL6D9uKft/yrmnxCogRERSONfq9NGWlfli+Q5avjSd8UuSc+QpsQoIEZHz5IqKZEDrK/ny0aaUL5qX/uOWcecbC9m2/6jfpWUqBYSISCquLBHHR72v4W+dqrHIOyX2tZk555RYBYSIyAVERhh3XlP+51Ni//nlGrqOmMPqHYf8Li3kFBAiImlQyjsldmjPOuw4cIyOQ2fxn6/XZuu7xCogRETSyMy4vmYppg5ozo11SzNi+kbavjKDORv2+l1aSCggRETSqWCeGP57Uy3evf9qAHq+Np8/f7SMg0dP+VxZxlJAiIhcpGsqFeXrFHeJbTkwkS+X78w2p8QqIERELkFsdCSPt72KCX0aU7JALA+/u5j7317EzoPH/C7tkikgREQyQLVSBRj/0DU83b4qszbsofXAGYydt5WzZ7Pu3oQCQkQkg0RFRnB/s4pM7tec2mUL8pdPV9Lt1bls2H3E79IuigJCRCSDlSuSh7H3NuDFm2uxfvcR2g+ayeBp6zl5OmtdYKeAEBEJATPjpnplmDqgOddVv4yBU77j+iEzWfz9j36XlmYKCBGRECoWl4shPerw+p0JHD5+mhtHzOHZCas4ciL8e04oIEREMkHLqiWYMqA5dzS8nLfmbuG6l2fw7brdfpd1QQoIEZFMki9XFH/rXJ2Pejcid0wkd7+xkL7vL2HfkRN+lxaUAkJEJJPVu7wwXz7ahH6tKjNxxU5aDUzkk8Xh13NCASEi4oNcUZH0a3UlEx9tSoWieRnwwTLuGLMgrHpOKCBERHxU2es58ffO1Vi89cefe06cCYML7BQQIiI+i4gw7mgU6DnR6FzPieGzfe85oYAQEQkTpQrm5vU7ExjSow7JPx6j09BZvDDJv54TCggRkTBiZnSsFeg5cUOd0gz7diPtB81k3qZ9mV6LAkJEJAwVyhvDizfX4n/3Xs2ps2fpPmoeT36ynIPHMq/nhAJCRCSMNalclEn9mtGrWUXGLdxG64GJfL3yh0x5bQWEiEiYyxMTxVPtq/LZw00oki8Xvf+3iN5jF7Hr0PGQvm5IA8LM2prZOjPbYGZPBJlfwMw+N7NlZrbKzO5OMW+Lma0ws6VmlhTKOkVEsoIaZQowoU9jHm97Fd+u202rgYm8O//7kPWcCFlAmFkkMAxoB8QDPcws/rzFHgZWO+dqAS2Al8wsJsX8a51ztZ1zCaGqU0QkK4mOjODBFpX4ul8zqpXKz1PjV9B99DyOnsz4m/9FZfgz/qIBsME5twnAzN4HOgOrUyzjgDgzMyAfsB8I/1scioj4rELRvLx3f0M+SNrG4q0HyBOT8R/noQyI0sC2FN8nA1eft8xQYAKwA4gDbnHOneuo4YDJZuaAV51zo0JYq4hIlmNm3FK/HLfULxeS5w/lGIQFmXb+gbLrgKVAKaA2MNTM8nvzGjvn6hI4RPWwmTUL+iJmvcwsycyS9uzZkyGFi4hIaAMiGSib4vsyBPYUUrob+MQFbAA2A1cBOOd2eP/uBsYTOGT1G865Uc65BOdcQrFixTL4RxARyblCGRALgcpmVsEbeO5O4HBSSt8DLQHMrARQBdhkZnnNLM6bnhdoA6wMYa0iInKekI1BOOdOm1kfYBIQCYxxzq0ys97e/JHAP4A3zWwFgUNSjzvn9ppZRWB8YOyaKOBd59zXoapVRER+y8KtQcWlSEhIcElJumRCRCStzGxRapcS6EpqEREJSgEhIiJBKSBERCSobDUGYWZ7gK0XuXpRYG8GlpNRVFf6hWttqit9VFf6XUxtlzvngl4jkK0C4lKYWVI43vNJdaVfuNamutJHdaVfRtemQ0wiIhKUAkJERIJSQPwiXG8GqLrSL1xrU13po7rSL0Nr0xiEiIgEpT0IEREJSgEhIiJB5fiA+L2+2ZlYR1kz+9bM1nj9uft60581s+1eb+6lZtbep/p+0yPczAqb2RQzW+/9WyiTa6qSYrssNbNDZtbPj21mZmPMbLeZrUwxLdXtY2ZPeu+5dWZ2nQ+1vWBma81suZmNN7OC3vTyZnYsxbYbmcl1pfq7y6xtlkpd41LUtMXMlnrTM3N7pfYZEbr3mXMux34RuMvsRqAiEAMsA+J9qqUkUNd7HAd8R6CX97PAH8NgW20Bip437b/AE97jJ4D/+Py7/AG43I9tBjQD6gIrf2/7eL/XZUAuoIL3HozM5NraAFHe4/+kqK18yuV82GZBf3eZuc2C1XXe/JeAv/qwvVL7jAjZ+yyn70H83DfbOXcSONc3O9M553Y65xZ7jw8Dawi0bQ1nnYG3vMdvATf4VwotgY3OuYu9kv6SOOdmEOipnlJq26cz8L5z7oRzbjOwgVQaYoWqNufcZOfcuf7v8wg09MpUqWyz1GTaNrtQXRboQdANeC8Ur30hF/iMCNn7LKcHRLC+2b5/KJtZeaAOMN+b1Mc7FDAmsw/jpHCuR/giM+vlTSvhnNsJgTcvUNyn2iDQkCrlf9pw2GapbZ9we9/dA3yV4vsKZrbEzBLNrKkP9QT73YXLNmsK7HLOrU8xLdO313mfESF7n+X0gEhL3+xMZWb5gI+Bfs65Q8AIoBKBnt07Ceze+iFNPcL9YIGOhZ2AD71J4bLNUhM27zszexo4DbzjTdoJlHPO1QEGAO/aL33iM0Nqv7tw2WY9+PUfIpm+vYJ8RqS6aJBp6dpmOT0g0tI3O9OYWTSBX/w7zrlPAJxzu5xzZ5xzZ4HRhPBQxIW44D3Cd5lZSa/2ksBuP2ojEFqLnXO7vBrDYpuR+vYJi/edmd0JXA/c6ryD1t7hiH3e40UEjltfmVk1XeB35/s2M7MooCsw7ty0zN5ewT4jCOH7LKcHRFr6ZmcK79jm68Aa59zAFNNLplisCz705rbUe4RPAO70FrsT+Cyza/P86q+6cNhmntS2zwSgu5nlMrMKQGVgQWYWZmZtgceBTs65oymmFzOzSO9xRa+2TZlYV2q/O9+3GdAKWOucSz43ITO3V2qfEYTyfZYZo+/h/AW0J3A2wEbgaR/raEJg9285sNT7ag+MBVZ40ycAJX2orSKBsyGWAavObSegCDANWO/9W9iH2vIA+4ACKaZl+jYjEFA7gVME/nK790LbB3jae8+tA9r5UNsGAsenz73XRnrL3uj9jpcBi4GOmVxXqr+7zNpmwerypr8J9D5v2czcXql9RoTsfaZbbYiISFA5/RCTiIikQgEhIiJBKSBERCQoBYSIiASlgBARkaAUECJhwMxamNkXftchkpICQkREglJAiKSDmd1mZgu8e/+/amaRZnbEzF4ys8VmNs3MinnL1jazefZLz4VC3vQrzGyqmS3z1qnkPX0+M/vIAn0a3vGunBXxjQJCJI3MrCpwC4EbF9YGzgC3AnkJ3AuqLpAIPOOt8jbwuHOuJoGrg89NfwcY5pyrBVxD4KpdCNydsx+B+/hXBBqH+EcSuaAovwsQyUJaAvWAhd4f97kJ3BjtLL/cwO1/wCdmVgAo6JxL9Ka/BXzo3dOqtHNuPIBz7jiA93wLnHefH69jWXlgVsh/KpFUKCBE0s6At5xzT/5qotlfzlvuQvevudBhoxMpHp9B/z/FZzrEJJJ204CbzKw4/NwL+HIC/49u8pbpCcxyzh0EfkzRQOZ2INEF7t+fbGY3eM+Ry8zyZOYPIZJW+gtFJI2cc6vN7P8IdNaLIHC3z4eBn4BqZrYIOEhgnAICt14e6QXAJuBub/rtwKtm9nfvOW7OxB9DJM10N1eRS2RmR5xz+fyuQySj6RCTiIgEpT0IEREJSnsQIiISlAJCRESCUkCIiEhQCggREQlKASEiIkH9P7FqxPN7BDCnAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(history.history.keys())\n",
    "\n",
    "# Summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure above, it looks like our model is performing better and better as the number of training (epoch) increases, which is a good sign!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Evaluate our model</b>\n",
    "\n",
    "And finally, once we have a model that seems to train quite well, we can proceed to the evaluation stage. In this stage, we are going to ask the model to predict on the dataset of penguin we have never seen before (which is X_test). To do this, in TensorFlow, we can just call `.predict()` and TensorFlow itself will do the heavy lifting of predicting the dataset that we pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5345113  0.47786322 0.49171287]\n",
      " [0.50538385 0.47197738 0.5489449 ]\n",
      " [0.7116676  0.4192878  0.4321921 ]\n",
      " [0.57126695 0.4553468  0.4913654 ]\n",
      " [0.51118517 0.4676627  0.5489432 ]\n",
      " [0.5726141  0.45565468 0.4870718 ]\n",
      " [0.53534174 0.49313956 0.47672448]\n",
      " [0.52639073 0.46731538 0.5472154 ]\n",
      " [0.64711875 0.42866808 0.45303357]\n",
      " [0.65696526 0.43830952 0.45201814]]\n"
     ]
    }
   ],
   "source": [
    "y_output = model.predict(X_test)            # Predict X_test by \n",
    "print(y_output[:10])                        # Print the top 10 rows of the produced output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a moment and understand what does the produced output indicates.\n",
    "\n",
    "Remember that each row corresponds to a sample (Row 0 = Sample 0, Row 1 = Sample 1, ...). And from the output above, each row has an array with three elements. Each element actually represents the probability of how certain our neural network is with the corresponding label.\n",
    "\n",
    "Let's take a look at the array below\n",
    "        \n",
    "        [0.65591866 0.4409073  0.44767407]\n",
    "\n",
    "This array means that the probability of our NN to predict the sample to be 0 (Adelie) is ~0.655, and then to predict 1 (Chinstrap) is ~0.441, and to predict 2 (Gentoo) is ~0.448.\n",
    "From this, the logical thing to do is to take the label with the highest probability score to be labelled as predicted output. \n",
    "\n",
    "And now, our predicted output for this sample is Chinstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 0 0 2 0 0 2 0 0 0 0 2 0 0 0 2 2 2 0 0 0 0 0 2 0 0 0 2 2 0 2 0 0 0 0 0\n",
      " 0 0 2 0 0 0 2 2 2 2 0 0 2 2 0 0 0 2 2 0 2 0 0 2 0 2 2 0 0 0 2 0 0 0 2 0 2\n",
      " 0 0 0 2 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_predicted = np.argmax(y_output, axis=1)       # For each row, take the index of the element with the highest probability\n",
    "print(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0 0 2 0 1 2 0 0 0 0 2 0 1 0 2 2 2 0 1 0 0 0 2 1 1 0 2 2 0 2 0 1 0 1 1\n",
      " 0 1 2 0 0 0 2 2 2 2 1 1 2 2 0 0 0 2 2 0 2 1 1 2 0 2 2 0 0 0 2 1 1 0 2 0 2\n",
      " 0 0 0 2 1 2 0 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# We also need to convert our y_test so that it produces an array in which each element represents the index of the correct label\n",
    "y_expected = np.argmax(y_test, axis=1)\n",
    "print(y_expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is just to improve readability. What it does is basically convert all the embedding into the real label (which is the name of the species of penguin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_label(array):\n",
    "    result = []\n",
    "    for y in array:\n",
    "        if y == 0:\n",
    "            y = 'Adelie'\n",
    "        elif y == 1:\n",
    "            y = 'Chinstrap'\n",
    "        elif y == 2:\n",
    "            y = 'Gentoo'\n",
    "        result.append(y)\n",
    "    \n",
    "    return result\n",
    "\n",
    "y_predicted_label = convert_to_label(y_predicted)\n",
    "y_expected_label = convert_to_label(y_expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adelie', 'Gentoo', 'Adelie', 'Adelie', 'Gentoo', 'Adelie', 'Adelie', 'Gentoo', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Gentoo', 'Adelie', 'Adelie', 'Adelie', 'Gentoo', 'Gentoo', 'Gentoo', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Gentoo', 'Adelie', 'Adelie', 'Adelie', 'Gentoo', 'Gentoo', 'Adelie', 'Gentoo', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Gentoo', 'Adelie', 'Adelie', 'Adelie', 'Gentoo', 'Gentoo', 'Gentoo', 'Gentoo', 'Adelie', 'Adelie', 'Gentoo', 'Gentoo', 'Adelie', 'Adelie', 'Adelie', 'Gentoo', 'Gentoo', 'Adelie', 'Gentoo', 'Adelie', 'Adelie', 'Gentoo', 'Adelie', 'Gentoo', 'Gentoo', 'Adelie', 'Adelie', 'Adelie', 'Gentoo', 'Adelie', 'Adelie', 'Adelie', 'Gentoo', 'Adelie', 'Gentoo', 'Adelie', 'Adelie', 'Adelie', 'Gentoo', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie']\n"
     ]
    }
   ],
   "source": [
    "print(y_predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly let's seee how our Neural Network model performs by calculating the accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (using Neural Network):  0.7674418604651163\n"
     ]
    }
   ],
   "source": [
    "tf_accuracy = accuracy_score(y_predicted_label, y_expected_label)\n",
    "\n",
    "print(\"Accuracy (using Neural Network): \", tf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Further Resources\n",
    "\n",
    "- [TensorFlow Quickstart](https://www.tensorflow.org/tutorials/quickstart/beginner)\n",
    "- [TensorFlow Tutorial](https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/)\n",
    "- [Cheat Sheet](https://github.com/kailashahirwar/cheatsheets-ai/blob/master/PDFs/Tensorflow.pdf)\n",
    "- [Cheat Sheet](https://www.kaggle.com/getting-started/134439)\n",
    "- [Documentation](https://www.tensorflow.org/api_docs/python/tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 PyTorch\n",
    "\n",
    "- PyTorch is an optimized Deep Learning tensor library based on Python and Torch and is mainly used for applications using GPUs and CPUs. PyTorch is favored over other Deep Learning frameworks like TensorFlow and Keras since it uses dynamic computation graphs and is completely Pythonic. - Simplilearn\n",
    "\n",
    "- PyTorch is a machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing, originally developed by Meta AI and now part of the Linux Foundation umbrella. - Wikipedia\n",
    "\n",
    "- An open source machine learning framework that accelerates the path from research prototyping to production deployment. - PyTorch website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 Further Resources\n",
    "\n",
    "- [PyTorch Quickstart](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)\n",
    "- [Implement NN for Penguin Classification with PyTorch](https://github.com/rianrajagede/penguin-python/blob/master/Pytorch/penguin_pytorch.py)\n",
    "- [PyTorch vs TensorFlow](https://towardsdatascience.com/pytorch-vs-tensorflow-spotting-the-difference-25c75777377b)\n",
    "- [Cheat Sheet](https://www.stefanseegerer.de/media/pytorch-cheatsheet-EN.pdf)\n",
    "- [Documentation](https://pytorch.org/docs/stable/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('ClimateHack')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11841742d11f96be4de93e6e6c2ae1e3a22839abf86213b3c23c4bf0c62307b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
